{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Cost function is \n",
    "\\begin{equation}\n",
    "J(\\Theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\Theta(x^{(i)})-y^{(i)})^2\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Prediction function is\n",
    "\\begin{equation}\n",
    "h_\\Theta(x^{(i)}) = \\Theta_0 + \\Theta_1 x^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "Gradient Descent is\n",
    "\\begin{equation}\n",
    "\\nabla J(\\Theta) = (\\frac{\\delta J}{\\delta \\Theta_0}, \\frac{\\delta J}{\\delta \\Theta_1})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\delta J}{\\delta \\Theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\Theta(x^{(i)})-y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\delta J}{\\delta \\Theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\Theta(x^{(i)})-y^{(i)})x^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{X} = \n",
    "\\left[ \n",
    "\\begin{array}{ccc}\n",
    "1 & x^{(1)} \\\\\n",
    "1 & x^{(2)} \\\\\n",
    "1 & x^{(3)} \\\\\n",
    "... \\\\\n",
    "1 & x^{(m)}\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\Theta} = \n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\Theta_0 \\\\\n",
    "\\Theta_1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{Y} = \n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "y^{(3)} \\\\\n",
    "... \\\\\n",
    "y^{(m)} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Cost function is \n",
    "\\begin{equation}\n",
    "J(\\Theta) = \\frac{1}{2m} (\\vec{X} \\vec{\\Theta} - \\vec{Y})^T (\\vec{X} \\vec{\\Theta} - \\vec{Y})\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Gradient Descent is\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\delta J}{\\delta \\Theta_0} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\Theta(x^{(i)})-y^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\delta J}{\\delta \\Theta_1} = \\frac{1}{m} \\sum_{i=1}^{m} (h_\\Theta(x^{(i)})-y^{(i)})x^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla J(\\Theta) = (\\frac{\\delta J}{\\delta \\Theta_0}, \\frac{\\delta J}{\\delta \\Theta_1}) = \n",
    "\\frac{1}{m} \\vec{X}^T (\\vec{X} \\vec{\\Theta} - \\vec{Y})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 一共是20个测试点\n",
    "m = 20\n",
    "\n",
    "# x0为都是1的\n",
    "X0 = np.ones((m, 1), dtype=np.int)\n",
    "X0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1],\n",
       "       [ 2],\n",
       "       [ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 6],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [ 9],\n",
       "       [10],\n",
       "       [11],\n",
       "       [12],\n",
       "       [13],\n",
       "       [14],\n",
       "       [15],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [19],\n",
       "       [20]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x1为从1到20的数字\n",
    "X1 = np.arange(1, m+1).reshape(m, 1)\n",
    "X1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\vec{X} = \n",
    "\\left[ \n",
    "\\begin{array}{ccc}\n",
    "1 & x^{(1)} \\\\\n",
    "1 & x^{(2)} \\\\\n",
    "1 & x^{(3)} \\\\\n",
    "... \\\\\n",
    "1 & x^{(m)}\n",
    "\\end{array} \n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1],\n",
       "       [ 1,  2],\n",
       "       [ 1,  3],\n",
       "       [ 1,  4],\n",
       "       [ 1,  5],\n",
       "       [ 1,  6],\n",
       "       [ 1,  7],\n",
       "       [ 1,  8],\n",
       "       [ 1,  9],\n",
       "       [ 1, 10],\n",
       "       [ 1, 11],\n",
       "       [ 1, 12],\n",
       "       [ 1, 13],\n",
       "       [ 1, 14],\n",
       "       [ 1, 15],\n",
       "       [ 1, 16],\n",
       "       [ 1, 17],\n",
       "       [ 1, 18],\n",
       "       [ 1, 19],\n",
       "       [ 1, 20]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.hstack((X0, X1))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\vec{y} = \n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "y^{(1)} \\\\\n",
    "y^{(2)} \\\\\n",
    "y^{(3)} \\\\\n",
    "... \\\\\n",
    "y^{(m)} \\\\\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3],\n",
       "       [ 4],\n",
       "       [ 5],\n",
       "       [ 5],\n",
       "       [ 2],\n",
       "       [ 4],\n",
       "       [ 7],\n",
       "       [ 8],\n",
       "       [11],\n",
       "       [ 8],\n",
       "       [12],\n",
       "       [11],\n",
       "       [13],\n",
       "       [13],\n",
       "       [16],\n",
       "       [17],\n",
       "       [18],\n",
       "       [17],\n",
       "       [19],\n",
       "       [21]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y点的值\n",
    "y = np.array([\n",
    "    3, 4, 5, 5, 2, 4, 7, 8, 11, 8, 12,\n",
    "    11, 13, 13, 16, 17, 18, 17, 19, 21\n",
    "]).reshape(m, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率用 alpha 表示\n",
    "alpha = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们以矩阵向量的形式定义代价函数和代价函数的梯度\n",
    "\n",
    "Cost function is \n",
    "\\begin{equation}\n",
    "J(\\Theta) = \\frac{1}{2m} (\\vec{X} \\vec{\\Theta} - \\vec{y})^T (\\vec{X} \\vec{\\Theta} - \\vec{y})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function(theta, X, y):\n",
    "    diff = np.dot(X, theta) - y\n",
    "    return (1./2/m) * np.dot(np.transpose(diff), diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\nabla J(\\Theta) = (\\frac{\\delta J}{\\delta \\Theta_0}, \\frac{\\delta J}{\\delta \\Theta_1}) = \n",
    "\\frac{1}{m} \\vec{X}^T (\\vec{X} \\vec{\\Theta} - \\vec{Y})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_function(theta, X, y):\n",
    "    diff = np.dot(X, theta) - y\n",
    "    return (1./m) * np.dot(np.transpose(X), diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后就是算法的核心部分，梯度下降迭代计算\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{\\Theta} = \n",
    "\\left[\n",
    "\\begin{array}{ccc}\n",
    "\\Theta_0 \\\\\n",
    "\\Theta_1\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, alpha):\n",
    "    # 给一个初始的值\n",
    "    theta = np.array([1, 1]).reshape(2, 1)\n",
    "    gradient = gradient_function(theta, X, y)\n",
    "    while not np.all(np.absolute(gradient) <= 1e-5):\n",
    "        theta = theta - alpha * gradient\n",
    "        gradient = gradient_function(theta, X, y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当梯度小于1e-5时，说明已经进入了比较平滑的状态，类似于山谷的状态，这时候再继续迭代效果也不大了，所以这个时候可以退出循环！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal: [[0.51583286]\n",
      " [0.96992163]]\n",
      "error function: 1.0149624062331013\n"
     ]
    }
   ],
   "source": [
    "optimal = gradient_descent(X, y, alpha)\n",
    "print('optimal:', optimal)\n",
    "print('error function:', error_function(optimal, X, y)[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction function is\n",
    "\\begin{equation}\n",
    "h_\\Theta(x^{(i)}) = 0.516 + 0.970 x^{(i)}\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
